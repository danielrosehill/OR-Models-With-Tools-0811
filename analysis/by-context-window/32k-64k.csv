model_name,model_id,vendor,context_length,input_price_usd_per_m,output_price_usd_per_m,description
Qwen: Qwen3 4B (free),qwen/qwen3-4b:free,qwen,40960,0.0,0.0,"Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks. It introduces a dual-mode architecture—thinking and non-thinking—allowing dynamic switching between high-precision logical reasoning and efficient dialogue generation. This makes it well-suited for multi-turn chat, instruction following, and complex agent workflows."
Qwen: Qwen3 235B A22B (free),qwen/qwen3-235b-a22b:free,qwen,40960,0.0,0.0,"Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a ""thinking"" mode for complex reasoning, math, and code tasks, and a ""non-thinking"" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling."
Mistral: Mistral 7B Instruct (free),mistralai/mistral-7b-instruct:free,mistralai,32768,0.0,0.0,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.  *Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*"
Mistral: Mistral 7B Instruct,mistralai/mistral-7b-instruct,mistralai,32768,0.03,0.05,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.  *Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*"
Mistral: Mistral 7B Instruct v0.3,mistralai/mistral-7b-instruct-v0.3,mistralai,32768,0.03,0.05,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.  An improved version of [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2), with the following changes:  - Extended vocabulary to 32768 - Supports v3 Tokenizer - Supports function calling  NOTE: Support for function calling depends on the provider."
Qwen: Qwen2.5 7B Instruct,qwen/qwen-2.5-7b-instruct,qwen,32768,0.04,0.1,"Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:  - Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.  - Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.  - Long-context Support up to 128K tokens and can generate up to 8K tokens.  - Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.  Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)."
Qwen: Qwen3 14B,qwen/qwen3-14b,qwen,40960,0.05,0.22,"Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both complex reasoning and efficient dialogue. It supports seamless switching between a ""thinking"" mode for tasks like math, programming, and logical inference, and a ""non-thinking"" mode for general-purpose conversation. The model is fine-tuned for instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling."
Qwen: Qwen3 32B,qwen/qwen3-32b,qwen,40960,0.05,0.2,"Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both complex reasoning and efficient dialogue. It supports seamless switching between a ""thinking"" mode for tasks like math, coding, and logical inference, and a ""non-thinking"" mode for faster, general-purpose conversation. The model demonstrates strong performance in instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling. "
Mistral: Mistral Small 3,mistralai/mistral-small-24b-instruct-2501,mistralai,32768,0.05,0.08,"Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment.  The model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware. [Read the blog post about the model here.](https://mistral.ai/news/mistral-small-3/)"
Qwen: Qwen3 30B A3B,qwen/qwen3-30b-a3b,qwen,40960,0.06,0.22,"Qwen3, the latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance.  Significantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. The Qwen3-30B-A3B variant includes 30.5 billion parameters (3.3 billion activated), 48 layers, 128 experts (8 activated per task), and supports up to 131K token contexts with YaRN, setting a new standard among open-source models."
Qwen2.5 72B Instruct,qwen/qwen-2.5-72b-instruct,qwen,32768,0.07,0.26,"Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:  - Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.  - Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.  - Long-context Support up to 128K tokens and can generate up to 8K tokens.  - Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.  Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)."
Mistral: Pixtral 12B,mistralai/pixtral-12b,mistralai,32768,0.1,0.1,"The first multi-modal, text+image-to-text model from Mistral AI. Its weights were launched via torrent: https://x.com/mistralai/status/1833758285167722836."
Nous: DeepHermes 3 Mistral 24B Preview,nousresearch/deephermes-3-mistral-24b-preview,nousresearch,32768,0.15,0.59,"DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on Mistral-Small-24B, designed for chat, function calling, and advanced multi-turn reasoning. It introduces a dual-mode system that toggles between intuitive chat responses and structured “deep reasoning” mode using special system prompts. Fine-tuned via distillation from R1, it supports structured output (JSON mode) and function call syntax for agent-based applications.  DeepHermes 3 supports a **reasoning toggle via system prompt**, allowing users to switch between fast, intuitive responses and deliberate, multi-step reasoning. When activated with the following specific system instruction, the model enters a *""deep thinking""* mode—generating extended chains of thought wrapped in `<think></think>` tags before delivering a final answer.   System Prompt: You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem. "
Qwen: QwQ 32B,qwen/qwq-32b,qwen,32768,0.15,0.4,"QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini."
TheDrummer: Rocinante 12B,thedrummer/rocinante-12b,thedrummer,32768,0.17,0.43,Rocinante 12B is designed for engaging storytelling and rich prose.  Early testers have reported: - Expanded vocabulary with unique and expressive word choices - Enhanced creativity for vivid narratives - Adventure-filled and captivating stories
Qwen: Qwen3 235B A22B,qwen/qwen3-235b-a22b,qwen,40960,0.18,0.54,"Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a ""thinking"" mode for complex reasoning, math, and code tasks, and a ""non-thinking"" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling."
Mistral: Saba,mistralai/mistral-saba,mistralai,32768,0.2,0.6,"Mistral Saba is a 24B-parameter language model specifically designed for the Middle East and South Asia, delivering accurate and contextually relevant responses while maintaining efficient performance. Trained on curated regional datasets, it supports multiple Indian-origin languages—including Tamil and Malayalam—alongside Arabic. This makes it a versatile option for a range of regional and multilingual applications. Read more at the blog post [here](https://mistral.ai/en/news/mistral-saba)"
Mistral Small,mistralai/mistral-small,mistralai,32768,0.2,0.6,"With 22 billion parameters, Mistral Small v24.09 offers a convenient mid-point between (Mistral NeMo 12B)[/mistralai/mistral-nemo] and (Mistral Large 2)[/mistralai/mistral-large], providing a cost-effective solution that can be deployed across various platforms and environments. It has better reasoning, exhibits more capabilities, can produce and reason about code, and is multiligual, supporting English, French, German, Italian, and Spanish."
Mistral Tiny,mistralai/mistral-tiny,mistralai,32768,0.25,0.25,"Note: This model is being deprecated. Recommended replacement is the newer [Ministral 8B](/mistral/ministral-8b)  This model is currently powered by Mistral-7B-v0.2, and incorporates a ""better"" fine-tuning than [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial."
TheDrummer: UnslopNemo 12B,thedrummer/unslopnemo-12b,thedrummer,32768,0.4,0.4,"UnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing and role-play scenarios."
Mistral: Magistral Small 2506,mistralai/magistral-small-2506,mistralai,40000,0.5,1.5,"Magistral Small is a 24B parameter instruction-tuned model based on Mistral-Small-3.1 (2503), enhanced through supervised fine-tuning on traces from Magistral Medium and further refined via reinforcement learning. It is optimized for reasoning and supports a wide multilingual range, including over 20 languages."
Mistral: Mixtral 8x7B Instruct,mistralai/mixtral-8x7b-instruct,mistralai,32768,0.54,0.54,"Mixtral 8x7B Instruct is a pretrained generative Sparse Mixture of Experts, by Mistral AI, for chat and instruction use. Incorporates 8 experts (feed-forward networks) for a total of 47 billion parameters.  Instruct model fine-tuned by Mistral. #moe"
Sao10K: Llama 3.1 Euryale 70B v2.2,sao10k/l3.1-euryale-70b,sao10k,32768,0.65,0.75,Euryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b).
Meta: Llama 3.1 405B Instruct,meta-llama/llama-3.1-405b-instruct,meta-llama,32768,0.8,0.8,"The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.  Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.  It has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.  To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."
Deep Cogito: Cogito V2 Preview Llama 70B,deepcogito/cogito-v2-preview-llama-70b,deepcogito,32768,0.88,0.88,"Cogito v2 70B is a dense hybrid reasoning model that combines direct answering capabilities with advanced self-reflection. Built with iterative policy improvement, it delivers strong performance across reasoning tasks while maintaining efficiency through shorter reasoning chains and improved intuition."
Qwen: Qwen-Max ,qwen/qwen-max,qwen,32768,1.6,6.4,"Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), especially for complex multi-step tasks. It's a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. The parameter count is unknown."
Mistral: Magistral Medium 2506 (thinking),mistralai/magistral-medium-2506:thinking,mistralai,40960,2.0,5.0,Magistral is Mistral's first reasoning model. It is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical.
Mistral: Magistral Medium 2506,mistralai/magistral-medium-2506,mistralai,40960,2.0,5.0,Magistral is Mistral's first reasoning model. It is ideal for general purpose use requiring longer thought processing and better accuracy than with non-reasoning LLMs. From legal research and financial forecasting to software development and creative storytelling — this model solves multi-step challenges where transparency and precision are critical.
Deep Cogito: Cogito V2 Preview Llama 405B,deepcogito/cogito-v2-preview-llama-405b,deepcogito,32768,3.5,3.5,Cogito v2 405B is a dense hybrid reasoning model that combines direct answering capabilities with advanced self-reflection. It represents a significant step toward frontier intelligence with dense architecture delivering performance competitive with leading closed models. This advanced reasoning system combines policy improvement with massive scale for exceptional capabilities. 
