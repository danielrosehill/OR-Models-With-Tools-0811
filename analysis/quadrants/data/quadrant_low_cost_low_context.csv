model_name,model_id,vendor,context_length,input_price_usd_per_m,output_price_usd_per_m,avg_cost,quadrant,value_score,description
Mistral: Mistral Small 3.1 24B,mistralai/mistral-small-3.1-24b-instruct,mistralai,128000,0.05,0.1,0.07500000000000001,Low Cost / Low Context,1706666.6666666665,"Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)"
Mistral: Devstral Small 2505,mistralai/devstral-small-2505,mistralai,128000,0.06,0.12,0.09,Low Cost / Low Context,1422222.2222222222,"Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3.1, jointly developed by Mistral AI and All Hands AI for advanced software engineering tasks. It is optimized for codebase exploration, multi-file editing, and integration into coding agents, achieving state-of-the-art results on SWE-Bench Verified (46.8%).  Devstral supports a 128k context window and uses a custom Tekken tokenizer. It is text-only, with the vision encoder removed, and is suitable for local deployment on high-end consumer hardware (e.g., RTX 4090, 32GB RAM Macs). Devstral is best used in agentic workflows via the OpenHands scaffold and is compatible with inference frameworks like vLLM, Transformers, and Ollama. It is released under the Apache 2.0 license."
Qwen: Qwen3 8B,qwen/qwen3-8b,qwen,128000,0.04,0.14,0.09000000000000001,Low Cost / Low Context,1422222.222222222,"Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between ""thinking"" mode for math, coding, and logical inference, and ""non-thinking"" mode for general conversation. The model is fine-tuned for instruction-following, agent integration, creative writing, and multilingual use across 100+ languages and dialects. It natively supports a 32K token context window and can extend to 131K tokens with YaRN scaling."
Amazon: Nova Micro 1.0,amazon/nova-micro-v1,amazon,128000,0.04,0.14,0.09000000000000001,Low Cost / Low Context,1422222.222222222,"Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat, and brainstorming. It has  simple mathematical reasoning and coding abilities."
Microsoft: Phi-3.5 Mini 128K Instruct,microsoft/phi-3.5-mini-128k-instruct,microsoft,128000,0.1,0.1,0.1,Low Cost / Low Context,1280000.0,"Phi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include both synthetic data and the filtered, publicly available websites data, with a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only transformer model using the same tokenizer as [Phi-3 Mini](/models/microsoft/phi-3-mini-128k-instruct).  The models underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks that test common sense, language understanding, math, code, long context and logical reasoning, Phi-3.5 models showcased robust and state-of-the-art performance among models with less than 13 billion parameters."
Z.AI: GLM 4 32B ,z-ai/glm-4-32b,z-ai,128000,0.1,0.1,0.1,Low Cost / Low Context,1280000.0,"GLM 4 32B is a cost-effective foundation language model.  It can efficiently perform complex tasks and has significantly enhanced capabilities in tool use, online search, and code-related intelligent tasks.  It is made by the same lab behind the thudm models."
Microsoft: Phi-3 Mini 128K Instruct,microsoft/phi-3-mini-128k-instruct,microsoft,128000,0.1,0.1,0.1,Low Cost / Low Context,1280000.0,"Phi-3 Mini is a powerful 3.8B parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.  At time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. This model is static, trained on an offline dataset with an October 2023 cutoff date."
Mistral: Mistral 7B Instruct v0.3,mistralai/mistral-7b-instruct-v0.3,mistralai,32768,0.03,0.05,0.04,Low Cost / Low Context,819200.0,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.  An improved version of [Mistral 7B Instruct v0.2](/models/mistralai/mistral-7b-instruct-v0.2), with the following changes:  - Extended vocabulary to 32768 - Supports v3 Tokenizer - Supports function calling  NOTE: Support for function calling depends on the provider."
Mistral: Mistral 7B Instruct,mistralai/mistral-7b-instruct,mistralai,32768,0.03,0.05,0.04,Low Cost / Low Context,819200.0,"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.  *Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*"
Mistral: Devstral Small 1.1,mistralai/devstral-small,mistralai,128000,0.07,0.28,0.17500000000000002,Low Cost / Low Context,731428.5714285714,"Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, developed by Mistral AI in collaboration with All Hands AI. Finetuned from Mistral Small 3.1 and released under the Apache 2.0 license, it features a 128k token context window and supports both Mistral-style function calling and XML output formats.  Designed for agentic coding workflows, Devstral Small 1.1 is optimized for tasks such as codebase exploration, multi-file edits, and integration into autonomous development agents like OpenHands and Cline. It achieves 53.6% on SWE-Bench Verified, surpassing all other open models on this benchmark, while remaining lightweight enough to run on a single 4090 GPU or Apple silicon machine. The model uses a Tekken tokenizer with a 131k vocabulary and is deployable via vLLM, Transformers, Ollama, LM Studio, and other OpenAI-compatible runtimes. "
Baidu: ERNIE 4.5 21B A3B,baidu/ernie-4.5-21b-a3b,baidu,120000,0.07,0.28,0.17500000000000002,Low Cost / Low Context,685714.2857142857,"A sophisticated text-based Mixture-of-Experts (MoE) model featuring 21B total parameters with 3B activated per token, delivering exceptional multimodal understanding and generation through heterogeneous MoE structures and modality-isolated routing. Supporting an extensive 131K token context length, the model achieves efficient inference via multi-expert parallel collaboration and quantization, while advanced post-training techniques including SFT, DPO, and UPO ensure optimized performance across diverse applications with specialized routing and balancing losses for superior task handling."
Mistral: Mistral Small 3,mistralai/mistral-small-24b-instruct-2501,mistralai,32768,0.05,0.08,0.065,Low Cost / Low Context,504123.0769230769,"Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment.  The model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware. [Read the blog post about the model here.](https://mistral.ai/news/mistral-small-3/)"
Qwen: Qwen2.5 7B Instruct,qwen/qwen-2.5-7b-instruct,qwen,32768,0.04,0.1,0.07,Low Cost / Low Context,468114.2857142857,"Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:  - Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.  - Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.  - Long-context Support up to 128K tokens and can generate up to 8K tokens.  - Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.  Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)."
OpenAI: GPT-4o-mini,openai/gpt-4o-mini,openai,128000,0.15,0.6,0.375,Low Cost / Low Context,341333.3333333333,"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.  As their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.  GPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).  Check out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.  #multimodal"
Cohere: Command R (08-2024),cohere/command-r-08-2024,cohere,128000,0.15,0.6,0.375,Low Cost / Low Context,341333.3333333333,"command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.  Read the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).  Use of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement)."
OpenAI: GPT-4o-mini (2024-07-18),openai/gpt-4o-mini-2024-07-18,openai,128000,0.15,0.6,0.375,Low Cost / Low Context,341333.3333333333,"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.  As their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.  GPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).  Check out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.  #multimodal"
Mistral: Pixtral 12B,mistralai/pixtral-12b,mistralai,32768,0.1,0.1,0.1,Low Cost / Low Context,327680.0,"The first multi-modal, text+image-to-text model from Mistral AI. Its weights were launched via torrent: https://x.com/mistralai/status/1833758285167722836."
Qwen: Qwen3 32B,qwen/qwen3-32b,qwen,40960,0.05,0.2,0.125,Low Cost / Low Context,327680.0,"Qwen3-32B is a dense 32.8B parameter causal language model from the Qwen3 series, optimized for both complex reasoning and efficient dialogue. It supports seamless switching between a ""thinking"" mode for tasks like math, coding, and logical inference, and a ""non-thinking"" mode for faster, general-purpose conversation. The model demonstrates strong performance in instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling. "
Qwen: Qwen3 14B,qwen/qwen3-14b,qwen,40960,0.05,0.22,0.135,Low Cost / Low Context,303407.4074074074,"Qwen3-14B is a dense 14.8B parameter causal language model from the Qwen3 series, designed for both complex reasoning and efficient dialogue. It supports seamless switching between a ""thinking"" mode for tasks like math, programming, and logical inference, and a ""non-thinking"" mode for general-purpose conversation. The model is fine-tuned for instruction-following, agent tool use, creative writing, and multilingual tasks across 100+ languages and dialects. It natively handles 32K token contexts and can extend to 131K tokens using YaRN-based scaling."
Qwen: Qwen3 30B A3B,qwen/qwen3-30b-a3b,qwen,40960,0.06,0.22,0.14,Low Cost / Low Context,292571.4285714285,"Qwen3, the latest generation in the Qwen large language model series, features both dense and mixture-of-experts (MoE) architectures to excel in reasoning, multilingual support, and advanced agent tasks. Its unique ability to switch seamlessly between a thinking mode for complex reasoning and a non-thinking mode for efficient dialogue ensures versatile, high-quality performance.  Significantly outperforming prior models like QwQ and Qwen2.5, Qwen3 delivers superior mathematics, coding, commonsense reasoning, creative writing, and interactive dialogue capabilities. The Qwen3-30B-A3B variant includes 30.5 billion parameters (3.3 billion activated), 48 layers, 128 experts (8 activated per task), and supports up to 131K token contexts with YaRN, setting a new standard among open-source models."
Nous: Hermes 3 70B Instruct,nousresearch/hermes-3-llama-3.1-70b,nousresearch,65536,0.3,0.3,0.3,Low Cost / Low Context,218453.33333333334,"Hermes 3 is a generalist language model with many improvements over [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo), including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.  Hermes 3 70B is a competitive, if not superior finetune of the [Llama-3.1 70B foundation model](/models/meta-llama/llama-3.1-70b-instruct), focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.  The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills."
Inception: Mercury Coder,inception/mercury-coder,inception,128000,0.25,1.0,0.625,Low Cost / Low Context,204800.0,"Mercury Coder is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like Claude 3.5 Haiku and GPT-4o Mini while matching their performance. Mercury Coder's speed means that developers can stay in the flow while coding, enjoying rapid chat-based iteration and responsive code completion suggestions. On Copilot Arena, Mercury Coder ranks 1st in speed and ties for 2nd in quality. Read more in the [blog post here](https://www.inceptionlabs.ai/blog/introducing-mercury)."
Inception: Mercury,inception/mercury,inception,128000,0.25,1.0,0.625,Low Cost / Low Context,204800.0,"Mercury is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like GPT-4.1 Nano and Claude 3.5 Haiku while matching their performance. Mercury's speed enables developers to provide responsive user experiences, including with voice agents, search interfaces, and chatbots. Read more in the [blog post] (https://www.inceptionlabs.ai/blog/introducing-mercury) here. "
Qwen2.5 72B Instruct,qwen/qwen-2.5-72b-instruct,qwen,32768,0.07,0.26,0.165,Low Cost / Low Context,198593.9393939394,"Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:  - Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.  - Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.  - Long-context Support up to 128K tokens and can generate up to 8K tokens.  - Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.  Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)."
Meta: Llama 3 8B Instruct,meta-llama/llama-3-8b-instruct,meta-llama,8192,0.03,0.06,0.045,Low Cost / Low Context,182044.44444444444,"Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases.  It has demonstrated strong performance compared to leading closed-source models in human evaluations.  To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."
Mistral: Voxtral Small 24B 2507,mistralai/voxtral-small-24b-2507,mistralai,32000,0.1,0.3,0.2,Low Cost / Low Context,160000.0,"Voxtral Small is an enhancement of Mistral Small 3, incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, translation and audio understanding. Input audio is priced at $100 per million seconds."
Mistral Tiny,mistralai/mistral-tiny,mistralai,32768,0.25,0.25,0.25,Low Cost / Low Context,131072.0,"Note: This model is being deprecated. Recommended replacement is the newer [Ministral 8B](/mistral/ministral-8b)  This model is currently powered by Mistral-7B-v0.2, and incorporates a ""better"" fine-tuning than [Mistral 7B](/models/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial."
Qwen: QwQ 32B,qwen/qwq-32b,qwen,32768,0.15,0.4,0.275,Low Cost / Low Context,119156.36363636363,"QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini."
Qwen: Qwen3 235B A22B,qwen/qwen3-235b-a22b,qwen,40960,0.18,0.54,0.36,Low Cost / Low Context,113777.77777777778,"Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B parameters per forward pass. It supports seamless switching between a ""thinking"" mode for complex reasoning, math, and code tasks, and a ""non-thinking"" mode for general conversational efficiency. The model demonstrates strong reasoning ability, multilingual support (100+ languages and dialects), advanced instruction-following, and agent tool-calling capabilities. It natively handles a 32K token context window and extends up to 131K tokens using YaRN-based scaling."
TheDrummer: Rocinante 12B,thedrummer/rocinante-12b,thedrummer,32768,0.17,0.43,0.3,Low Cost / Low Context,109226.66666666667,Rocinante 12B is designed for engaging storytelling and rich prose.  Early testers have reported: - Expanded vocabulary with unique and expressive word choices - Enhanced creativity for vivid narratives - Adventure-filled and captivating stories
Nous: DeepHermes 3 Mistral 24B Preview,nousresearch/deephermes-3-mistral-24b-preview,nousresearch,32768,0.15,0.59,0.37,Low Cost / Low Context,88562.16216216216,"DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on Mistral-Small-24B, designed for chat, function calling, and advanced multi-turn reasoning. It introduces a dual-mode system that toggles between intuitive chat responses and structured “deep reasoning” mode using special system prompts. Fine-tuned via distillation from R1, it supports structured output (JSON mode) and function call syntax for agent-based applications.  DeepHermes 3 supports a **reasoning toggle via system prompt**, allowing users to switch between fast, intuitive responses and deliberate, multi-step reasoning. When activated with the following specific system instruction, the model enters a *""deep thinking""* mode—generating extended chains of thought wrapped in `<think></think>` tags before delivering a final answer.   System Prompt: You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem. "
Baidu: ERNIE 4.5 VL 28B A3B,baidu/ernie-4.5-vl-28b-a3b,baidu,30000,0.14,0.56,0.35000000000000003,Low Cost / Low Context,85714.28571428571,"A powerful multimodal Mixture-of-Experts chat model featuring 28B total parameters with 3B activated per token, delivering exceptional text and vision understanding through its innovative heterogeneous MoE structure with modality-isolated routing. Built with scaling-efficient infrastructure for high-throughput training and inference, the model leverages advanced post-training techniques including SFT, DPO, and UPO for optimized performance, while supporting an impressive 131K context length and RLVR alignment for superior cross-modal reasoning and generation capabilities."
Cogito V2 Preview Llama 109B,deepcogito/cogito-v2-preview-llama-109b-moe,deepcogito,32767,0.18,0.59,0.385,Low Cost / Low Context,85109.09090909091,"An instruction-tuned, hybrid-reasoning Mixture-of-Experts model built on Llama-4-Scout-17B-16E. Cogito v2 can answer directly or engage an extended “thinking” phase, with alignment guided by Iterated Distillation & Amplification (IDA). It targets coding, STEM, instruction following, and general helpfulness, with stronger multilingual, tool-calling, and reasoning performance than size-equivalent baselines. The model supports long-context use (up to 10M tokens) and standard Transformers workflows. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)"
Mistral: Saba,mistralai/mistral-saba,mistralai,32768,0.2,0.6,0.4,Low Cost / Low Context,81920.0,"Mistral Saba is a 24B-parameter language model specifically designed for the Middle East and South Asia, delivering accurate and contextually relevant responses while maintaining efficient performance. Trained on curated regional datasets, it supports multiple Indian-origin languages—including Tamil and Malayalam—alongside Arabic. This makes it a versatile option for a range of regional and multilingual applications. Read more at the blog post [here](https://mistral.ai/en/news/mistral-saba)"
TheDrummer: UnslopNemo 12B,thedrummer/unslopnemo-12b,thedrummer,32768,0.4,0.4,0.4,Low Cost / Low Context,81920.0,"UnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing and role-play scenarios."
Mistral Small,mistralai/mistral-small,mistralai,32768,0.2,0.6,0.4,Low Cost / Low Context,81920.0,"With 22 billion parameters, Mistral Small v24.09 offers a convenient mid-point between (Mistral NeMo 12B)[/mistralai/mistral-nemo] and (Mistral Large 2)[/mistralai/mistral-large], providing a cost-effective solution that can be deployed across various platforms and environments. It has better reasoning, exhibits more capabilities, can produce and reason about code, and is multiligual, supporting English, French, German, Italian, and Spanish."
Mistral: Mixtral 8x7B Instruct,mistralai/mixtral-8x7b-instruct,mistralai,32768,0.54,0.54,0.54,Low Cost / Low Context,60681.481481481474,"Mixtral 8x7B Instruct is a pretrained generative Sparse Mixture of Experts, by Mistral AI, for chat and instruction use. Incorporates 8 experts (feed-forward networks) for a total of 47 billion parameters.  Instruct model fine-tuned by Mistral. #moe"
Sao10K: Llama 3.1 Euryale 70B v2.2,sao10k/l3.1-euryale-70b,sao10k,32768,0.65,0.75,0.7,Low Cost / Low Context,46811.42857142857,Euryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.1](/models/sao10k/l3-euryale-70b).
Meta: Llama 3 70B Instruct,meta-llama/llama-3-70b-instruct,meta-llama,8192,0.3,0.4,0.35,Low Cost / Low Context,23405.714285714286,"Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases.  It has demonstrated strong performance compared to leading closed-source models in human evaluations.  To read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/)."
Mistral: Mistral 7B Instruct v0.1,mistralai/mistral-7b-instruct-v0.1,mistralai,2824,0.11,0.19,0.15,Low Cost / Low Context,18826.666666666668,"A 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, with optimizations for speed and context length."
